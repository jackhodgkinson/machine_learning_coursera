{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning, Reccomender Systems and Reinforcement Learning \n",
    "\n",
    "## Unsupervised Learning \n",
    "\n",
    "With supervised learning, we had a training set in the form $\\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\\}$ with $x^{(m)}$ as the input feature and the target output $y^{(m)}$.\n",
    "\n",
    "However in unsupervised learning, we have a training set in the form $\\{x^{(1)},...,x^{(m)}\\}$. We do not have the target labels $y$, so we cannot tell the algorithm what the \"right answer\" is. We are instead asking the algorithm what is interesting about the data. \n",
    "\n",
    "### Clustering \n",
    "\n",
    "#### What is clustering? \n",
    "\n",
    "A clustering algorithm looks at a variety of data points and automatically finds other data points that are related or similar. The clustering algorithm will try to arrange the data into groups or \"clusters\" where the data has similar properties. \n",
    "\n",
    "Clustering has many applications such as: \n",
    "* Grouping similar news.\n",
    "* Market segmentation. \n",
    "* DNA analysis. \n",
    "* Astronomical data analysis. \n",
    "\n",
    "#### K-means algorithm \n",
    "\n",
    "The steps of the K-means algorithm on training data $\\{x^{(1)},...,x^{(m)}\\}$ are: \n",
    "\n",
    "1. Randomly initialise $K$ cluster centroids $\\mu_1, \\mu_2, ... , \\mu_K$. $\\mu_K$ has the same dimension as the training data. \n",
    "2. Repeat: \n",
    "    * Assign points to cluster centroids\n",
    "        \n",
    "        **for $i=1$ to $m$:**\n",
    "        \n",
    "        $$c^{(i)} = \\min_{K} || x^{(i)} - \\mu_{K}||^2$$\n",
    "    \n",
    "    * Move cluster centroids\n",
    "\n",
    "        **for $k=1$ to $K$:**\n",
    "\n",
    "        $$\\mu_{K} = \\sum_{x^{(i)} \\epsilon  \\hspace{0.05cm} c^{(i)}} x^{(i)}$$\n",
    "\n",
    "        If no points are assigned to the new cluster, let $K=K-1$   \n",
    "\n",
    "##### Optimisation Objective of K-means\n",
    "\n",
    "As we have $c^{(i)}$ = index of cluster ($1,...,K$) to which example $x^{(i)}$ is currently assigned, and $\\mu_{K}$ = cluster centroid $K$. \n",
    "\n",
    "Let $\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned. \n",
    "\n",
    "K means is minimising the cost function:\n",
    "$$\\min_{c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K} J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K) = \\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)}-\\mu_{c^{(i)}}||^2$$\n",
    "\n",
    "##### Initialising K-means \n",
    "\n",
    "How do you take the random guess of assigning which data points will be the cluster centroids? \n",
    "\n",
    "When running K-means, you should undertake a process called random initialisation. The steps are:\n",
    "1.   Choose the number of cluster centroids $K$ to be less than the training examples $m$ i.e. $K<m$\n",
    "2. Randomly pick K training examples\n",
    "3. Set $\\mu_1,...,\\mu_K$ equal to these K examples. \n",
    "4. Run K-means $n$ times (where $50 < n < 1000$) to get $c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K$\n",
    "5. Computer cost function $J(c^{(1)},...,c^{(m)},\\mu_1,...,\\mu_K)$\n",
    "6. Pick the set of clusters that gave the lowest cost $J$\n",
    "\n",
    "##### Choosing the number of clusters\n",
    "\n",
    "To choose the value of $K$, you can evaluate K-means based on how well it performs on some later (downstream) purpose. \n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "Anomaly detection algorithms looks at an unlabeled dataset of normal events and thereby learns to detect unusual or anomalous events. \n",
    "\n",
    "#### Finding Unusual Events\n",
    "\n",
    "We can detect unusual / anomalous data using density estimation. Given a training dataset $\\{x^{(1)},...,x^{(m)}\\}$, we build a model $p(x)$ which is the probability of $x$.\n",
    "\n",
    "From the training set, we need to decide which regions are high and low probability regions. If $p(x_{test}) < \\epsilon$ (if probability is very small) then we can classify the event as an anomaly, whereas if $p(x_{test}) \\ge \\epsilon$ then we can classify the event as normal. \n",
    "\n",
    "Anomaly detection has a wide range of applications, such as fraud detection, manufacturing and monitoring computers in a data center.\n",
    "\n",
    "#### Gaussian (normal) distribution\n",
    "\n",
    "For a number $x$, the probabiltiy of $x p(x)$ is determined by a Gaussian/Normal distrinbution with mean $\\mu$ and variance $\\sigma^2$ with the following formula: \n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "The distribution can be seen on the picture below: \n",
    "\n",
    "![NDist](https://i0.wp.com/www.eajohansson.net/wp-content/uploads/2018/07/standard-normal-distribution-curve.jpg?resize=768%2C400&ssl=1)\n",
    "\n",
    "Where the area under the bell curve $=1$.\n",
    "\n",
    "#### Anomaly detection algorithm \n",
    "\n",
    "For a training set $\\{\\vec{x}^{(1)},...,\\vec{x}^{(1)}\\}$ where each example $\\vec{x}^{(i)}$ has $n$ features, we carry out density estimation. \n",
    "\n",
    "We calculate the probability $p(\\vec{x})$ as: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\vec{x}) & = p(x_1; \\mu_1, \\sigma^2_1) * p(x_2; \\mu_2, \\sigma^2_2) * ... * p(x_n; \\mu_n, \\sigma^2_n) \\\\ \n",
    "& = \\prod_{j=1}^{n} p(x_j;\\mu_j,\\sigma_j^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The steps of the algorithm are as follows: \n",
    "\n",
    "1. Choose $n$ features $x_i$ that you think might be indicative of anomalous examples. \n",
    "2. Fit parameters $\\mu_1, ..., \\mu_n, \\sigma^2_1,...,\\sigma^2_n$ where $\\mu_j = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)}$ and $\\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_j^{(i)}-\\mu_j)^2$. *Note: these formulas can be adapted for vectors.*\n",
    "3. Given new example $x$, compute $p(x) = \\prod_{j=1}^{n} p(x_j;\\mu_j,\\sigma_j^2) = \\prod_{j=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma_j} \\exp{-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2}}$\n",
    "4. If $p(x) < \\epsilon$ then we classify the feature as an anomaly.\n",
    "\n",
    "#### Developing and evaluating an anomaly detection system\n",
    "\n",
    "When developing a learning algorithm, making decisions is much easier if we have a way of evaluating our learning algorithm. \n",
    "\n",
    "Assume we have some labeled data, of anomalous $(y=0)$ and non-anomalous $(y=1)$ examples. \n",
    "\n",
    "We have a training set: $\\{x^{(1)},...,x^{(m)}\\}$ (assumiing normal examples and not anomalous). We also have a cross validation set $\\{(x^{(1)}_{cv},y^{(1)}_{cv}),...,(x^{(m_{cv})}_{cv},y^{(m_{cv})}_{cv})\\}$ and test set $\\{(x^{(1)}_{test},y^{(1)}_{test}),...,(x^{(m_{test})}_{test},y^{(m_{test})}_{test})\\}$. It is likely that these will include a few anomalous examples ($y=1$) but mostly normal examples ($y=0$). We would train the algorithm on the training data set, then use the cross validation set to tune $\\epsilon$ and $x_j$ and finally we use the test data set to test our algorithm. Alternatively, if you have very few labeled anomalous examples, you can do this without a test data set. \n",
    "\n",
    "To evaluate your algorithm, firstly fit model $p(x)$ on training set $x^{(1)},...,x^{(m)}$ Then on a cross validation/text example $x$, predict: \n",
    "$$ y =\n",
    "\\begin{cases}\n",
    "1 \\text{ if } p(x) \\lt \\epsilon \\text{ (anomaly)} \\\\\n",
    "0 \\text{ if } p(x) \\ge \\epsilon \\text{ (normal)} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Positive evaluation metrics: \n",
    "* True positive, false positive, false negative, true negative\n",
    "* Precision/Recall \n",
    "* $F_1$-score\n",
    "\n",
    "#### Anomaly detection vs supervised learning\n",
    "\n",
    "|Anomaly detection|Supervised Learning|\n",
    "|-----------------|-------------------|\n",
    "|Very small number of positive exampels $(y=1)$ and a large number of negative ($y=0$) examples.|Large number of positive and negative examples|\n",
    "|Many different \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like as future anomalies likely to have different charcteristics.|Enough positive examples for algorithm to get a sense of what positive examples are like, future  positive examples are likely to be similar to ones in the training set.|\n",
    "|Examples: <ul><li>Fraud detection</li><li>Manufacturing - finding new previously unseen defects</li><li>Monitoring machines in a data centre</li></ul>|Examples: <ul><li>Email spam classification</li><li>Manufacturing - finding known, previously seen defects</li><li>Weather prediction </li><li>Disease Classification</li></ul>|\n",
    "\n",
    "#### Choosing what features to use\n",
    "\n",
    "One step to help your algorithm is to try and make sure the features you give it are Gaussian. For non-Gaussian features, you can often change them to give them properties similar to Gaussian features. This is through transforming the features and plotting all the features as a histogram to see if your data follows a Normal distribution. We would do this in Python with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mhist(x, bins\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(x, bins=50) #Bins is just the amount of bars you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to apply the transformation not only to the training set, but also to the cross validation and test data sets. \n",
    "\n",
    "If the algorithm does not work well on your cross validation set, then you can carry out an error analysis process. We want: \n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) \\ge \\epsilon & \\text{ large for normal examples } x \\\\\n",
    "p(x) \\lt \\epsilon & \\text{ small for anomalous examples } x \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The most common problems are: \n",
    "* $p(x)$ is comparable for normal and anomalous examples. \n",
    "* $p(x)$ is large for both\n",
    "\n",
    "## Reccomender Systems \n",
    "\n",
    "### Collaborative Filtering \n",
    "\n",
    "#### Making reccomendations \n",
    "\n",
    "In a typical reccomender systems, you will have some number of users and some number of items that you want to reccomend to the users. \n",
    "\n",
    "Let $n_u =$ number of users, $n_i =$ number of items, $r(i,j)=1$ if user $j$  has rated item $i$ and $0$ if user has not rated item $i$ and $y^{(i,j)}=$ rating given by user $j$ to item $i$ iff $r(i,j)=1$.\n",
    "\n",
    "#### Using per-item features \n",
    "\n",
    "If we have a feature vector $x^{(i)}$ for item $i$, and $w^{(j)}, b^{(j)}$ which are parameters for user $j$, we can predict a score/rating by using $w^{(j)} \\cdot x^{(i)} + b^{(j)}$.\n",
    "\n",
    "If we have $m^{(j)} =$ number of items rated by user $j$, we can learn $w^{(j)}, b^{(j)}$ through the following cost function for user $j$:\n",
    "$$ J(w^{(j)},b^{(j)})=\\frac{1}{2} \\sum_{i:r(i,j)=1} (w^{(j)} \\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2  + \\frac{\\lambda}{2} \\sum_{k=1}^n (w^{(j)}_k) ^2 $$\n",
    "\n",
    "To learn parameters $w^{(1)}, b^{(1)},...,w^{(n_u)}, b^{(n_u)}$ for all users: \n",
    "\n",
    "$$ \\min_{w^{(1)}, b^{(1)},...,w^{(n_u)}, b^{(n_u)}} J\\bigg(\\begin{matrix} w^{(1)},...,w^{(n_u)} \\\\ b^{(1)},...,b^{(n_u)} \\\\ \\end{matrix}\\bigg) = \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i,j)=1} (w^{(j)} \\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2  + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n (w^{(j)}_k) ^2 $$\n",
    "\n",
    "#### Collaborative filtering algorithm \n",
    "\n",
    "Given $w^{(1)}, b^{(1)},...,w^{(n_u)}, b^{(n_u)}$, to learn $x^{(i)}$:\n",
    "$$\\min_{w^{(1)}, b^{(1)},...,w^{(n_u)}, b^{(n_u)}} J\\bigg(\\begin{matrix} w^{(1)},...,w^{(n_u)} \\\\ b^{(1)},...,b^{(n_u)} \\\\ \\end{matrix}\\bigg) = \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i,j)=1} (w^{(j)} \\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2  + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n (w^{(j)}_k)$$\n",
    "\n",
    "To learn $x^{(1)},...,x^{(n_m)}$:\n",
    "$$ \\min_{x^{(1)},...,x^{(n_m)}} J(x^{(1)},...,x^{(n_m)}) = \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j)=1} (w^{(j)} \\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2  + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n (x^{(i)}_k) ^2 $$\n",
    "\n",
    "We can put the two cost functions together to get: \n",
    "\n",
    "$$\\min_{\\Bigg(\\begin{matrix} w^{(1)},...,w^{(n_u)} \\\\ b^{(1)},...,b^{(n_u)} \\\\ x^{(1)},...,x^{(n_m)} \\\\ \\end{matrix}\\Bigg)} J(w,b,x) = \\frac{1}{2} \\sum_{(i,j):r(i,j)=1} (w^{(j)} \\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2  + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n (w^{(j)}_k)^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^n (x^{(i)}_k)^2 $$\n",
    "\n",
    "You can use gradient descent to minimise the cost function $J(w,b,x)$. Recall gradient descent as: \n",
    "$$ \\text{repeat until convergence: } \\begin{cases} w_i^{(j)} = w_i^{(j)} - \\alpha \\frac{\\partial}{\\partial w_i^{(j)}} J(w,b,x) \\\\  b^{(j)} = b^{(j)} - \\alpha \\frac{\\partial}{\\partial b^{(j)}} J(w,b,x) \\\\ x_k^{(i)} = x_k^{(i)} - \\alpha \\frac{\\partial}{\\partial x_k^{(i)}} J(w,b,x) \\end{cases} $$\n",
    "\n",
    "#### Binary labels: favourites, likes and clicks\n",
    "\n",
    "You can generalise the collaborative filtering algorithm to a binary classification problem. Previously we predicted $y^{(i,j)}$ as $w^{(j)} \\cdot x^{(i)} + b^{(j)}$ but for binary labels, we predict that the probability of $y^{(i,j)} = 1$ is given by:  $$g(w^{(j)} \\cdot x^{(i)} + b^{(j)}) \\text{ where } g(z) = \\frac{1}{1+e^{-z}}$$ The loss function for binary labels $y^{(i,j)}$ where $f_{(w,b,x)}$ = $g(w^{(j)} \\cdot x^{(i)} + b^{(j)})$ now also becomes: $$L(f_{(w,b,x)}(x),y^{(i,j)}) = -y^{(i,j)} \\log{(f_{(w,b,x)}(x))} - (1-y^{(i,j)}) \\log{(1 - f_{(w,b,x)}(x))}$$ So our cost function becomes: $$J(w,b,x) = \\sum_{(i,j):r(i,j)=1} L(f_{(w,b,x)}(x),y^{(i,j)})$$\n",
    "\n",
    "### Reccomender systems implementation detail \n",
    "\n",
    "#### Mean normalisation \n",
    "\n",
    "Mean normalisation helps the collaborative filtering algorithm to come up ratings for items where users have not provided ratings.\n",
    "\n",
    "Mean normalisation involves taking the sum of all ratings (excluding missing ones) and dividing by the number of ratings then subtracting the average values from the given values. For example: \n",
    "\n",
    "$ \\begin{bmatrix} 5 & 5 & 0 & 0 & ? \\\\ 5 & ? & ? & 0 & ? \\\\ ? & 4 & 0 & ? & ? \\\\ 0 & 0 & 5 & 4 & ? \\\\ 0 & 0 & 5 & 0 & ? \\end{bmatrix}$ gives the average $\\mu = \\begin{bmatrix} 2.5 \\\\ 2.5 \\\\ 2 \\\\ 2.25 \\\\ 1.25 \\end{bmatrix}$ which when subtracted from the original ratings gives our new matrix $y^{(i,j)}_{norm} = \\begin{bmatrix} 2.5 & 2.5 & -2.5 & -2.5 & ? \\\\ 2.5 & ? & ? & -2.5 & ? \\\\ ? & 2 & -2 & ? & ? \\\\ -2.25 & -2.25 & 2.75 & 1.75 & ? \\\\ -1.25 & -1.25 & 3.75 & -1.25 & ? \\\\ \\end{bmatrix}$ to learn $w^{(j)}, b^{(j)}, x^{(i)}$.\n",
    "\n",
    "For user $j$ on item $i$ predict: $w^{(j)} \\cdot x^{(i)} + b^{(j)} + \\mu_i$\n",
    "\n",
    "#### Applyng Collaborative Filtering using TensorFlow\n",
    "\n",
    "Recall that the gradient descent algorithm is as follows: \n",
    "\n",
    "$$ \\text{repeat until convergence: } \\begin{cases} w = w - \\alpha \\frac{\\partial}{\\partial w} J(w,b) \\\\  b = b - \\alpha \\frac{\\partial}{\\partial b} J(w,b) \\end{cases} $$\n",
    "\n",
    "We can do this in Python using Tensorflow. See an example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a Custom Training Loop \n",
    "import tensorflow as tf\n",
    "w = tf.Variable(3.0) # The parameters we want to optimise\n",
    "x = 1.0\n",
    "y = 1.0 \n",
    "alpha = 0.01 \n",
    "\n",
    "iterations = 30\n",
    "for iter in range(iterations): \n",
    "    # Use TensorFlow's Gradient tape to record the steps used to compute the cost J, to enable auto differentiation\n",
    "    with tf.GradientTape() as tape: \n",
    "        fwb = w*x\n",
    "        costJ = (fwb - y)**2\n",
    "    \n",
    "    # Use gradient tape to calculate graidents of cost w.r.t parameter w.\n",
    "    [dJdw] = tape.gradient(costJ,[w])\n",
    "\n",
    "    #Run one step of gradient descent by updating value of w to reduce the cost. \n",
    "    w.assign_add(-alpha*dJdw) # tf.Variable requires special function to modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For collaborative filtering, we have $J(w,b,x)$ so instead we use the following gradient descent algorithm: \n",
    "\n",
    "$$ \\text{repeat until convergence: } \\begin{cases} w = w - \\alpha \\frac{\\partial}{\\partial w} J(w,b) \\\\  b = b - \\alpha \\frac{\\partial}{\\partial b} J(w,b) \\\\ x = x - \\alpha \\frac{\\partial}{\\partial x} J(w,b,x) \\end{cases} $$\n",
    "\n",
    "To use this in collaborative filtering in Python we use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement the Collaborative Filtering in TensorFlow we follow the following code:\n",
    "\n",
    "# Instantiate an optimiser \n",
    "optimiser = keras.optimizers.Adam(learning_rate=1e-1)\n",
    "\n",
    "iterations = 200\n",
    "for iter in range(iterations): \n",
    "    # Use TensorFlow's Gradient Tape to record operations used to copute cost\n",
    "    with  tf. GradientTape() as tape: \n",
    "\n",
    "        #Compute cost\n",
    "        cost_value=cofiCostFuncV(X,W,b,norm,R,num_users, num_items, lambda)\n",
    "    \n",
    "    # Use the gradient tape to autmatically retrieve the gradients of the trainable variables w.r.t the loss\n",
    "    grads = tape.gradient(cost_value,[X,W,b])\n",
    "\n",
    "    #Run one step of gradient descent by updating the value of the variables to minimise the loss\n",
    "    optimiser.apply_gradients(zip(grads,[X,W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding related items \n",
    "\n",
    "Often when online shopping, the webpage shows you similar or related items. For example, if shopping for books, it may show you other books with a similar theme or genre. How do websites do that? \n",
    "\n",
    "This can be done through the collaborative filtering algorithm. The features $x^{(i)}$ are quite hard to interpret. To find other items related to it, then try to find item $k$ with $x^{(k)}$ similar to $x^{(i)}$. We do this by finding the smallest $|| x^{(k)} - x^{(i)} ||^2$. These will be the related items. \n",
    "\n",
    "A limitation of collaborative filtering is the cold start problem. If there is a new item in your catalogue, how do you rank a new item if there are very few ratings? Also, for new users who have ranked a very low number of items, how do you show reasonable reccomendation? This can be helped through mean normalisation. \n",
    "\n",
    "### Content-based filtering \n",
    "\n",
    "#### Collaborative filtering vs Content-based filtering\n",
    "\n",
    "|Collaborative Filtering|Content-Based Filtering|\n",
    "|-----------------------|-----------------------|\n",
    "|Reccomend items to you based on ratings of users who gave similar ratings as you|Reccomend items to you based on features of user and item to find good match|\n",
    "\n",
    "We continue to use $r(i,j)=1$ if user $j$ has rated item $i$ and $y^{(i,j)}$ as the rating given by user $j$ on item $i$. \n",
    "\n",
    "To predfict the rating of user $j$ on item $i$, we use the following formula by computing $v^{(j)}_u$ from features $x^{(j)}_u$ and $v_i^{(i)}$ from features $x_i^{(i)}$ to get the formula $v^{(j)}_u \\cdot v_i^{(i)}$. **Note the vectors $v$ must be the same size!**\n",
    "\n",
    "#### Deep learning for content-based filtering\n",
    "\n",
    "For the rating prediction $v_u \\cdot v_i$, we calculate $g(v_u \\cdot v_i)$ to predict the probability that $y^{(i,j)}=1$.\n",
    "\n",
    "We compute a cost function $$J = \\sum_{(i,j):r(i,j=1}(v_u^{(i)} \\cdot v_i^{(i)} - y^{(i,j)})^2 + \\text{ neural network regularisation term.}$$\n",
    "\n",
    "To find items similar to item $i$: $||v_i^{(k)} - v_i^{(i)}||^2$\n",
    "\n",
    "#### Reccomending from a large catalogue\n",
    "\n",
    "Sometimes, systems have a lot of similar items. How do you do this efficiently computationally?\n",
    "\n",
    "A lot of large scale reccomender systems are in two steps: retrieval and ranking.\n",
    "* Retrieval: \n",
    "  * Generate large list of plausible item candidates.\n",
    "  * Combine retrieved items into list, removing duplicates and items already engaged with. \n",
    "  * Retrieving more items results in better performance but slower reccomendations\n",
    "  * To optimise the trade-off, carry out offline experiements to see if retrieving additional items results in more relevant recommendations ($p(y^{(i,j)}=1)$ of items displayed to user are higher).\n",
    "* Ranking: \n",
    "  * Take list retrieved and rank using learned model.\n",
    "  * Display ranked items to user.\n",
    "\n",
    "#### TensorFlow Implementation of Content-Based Filtering\n",
    "\n",
    "Recall that our code has started with a user neural network and an item neural network. Please view the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 07:51:33.129863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(32,activation='linear')\n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(32,activation='linear')\n",
    "])\n",
    "\n",
    "num_user_features = 10\n",
    "num_item_features = 45\n",
    "\n",
    "# Create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# Create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vi = item_NN(input_item)\n",
    "vi = tf.linalg.l2_normalize(vi, axis=1)\n",
    "\n",
    "# Measure the similarity of the two vector outputs\n",
    "output = tf.keras.layers.Dot(axes=1)([vu,vi])\n",
    "\n",
    "# Specify the inputs and outputs of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "# Specify the cost function\n",
    "cost_f = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis \n",
    "\n",
    "#### Reducing the number of features\n",
    "\n",
    "PCA is an unsupervised learning method used for visualisation. This algorithm helps to reduce the number of features by looking at the range of values each feature presents, use fewer numbers and to find new axes and co-ordinates in order to plot the data. \n",
    "\n",
    "#### PCA Algorithm \n",
    "\n",
    "Principal Component Analysis involves performing feature scaling and then fitting the data to obtain 2 or 3 new axes called principal components that are all perpendicular to each other. The steps are: \n",
    "\n",
    "1. Optional pre-processing: perform feature scaling.\n",
    "    * Fit the data to obtain 2 (or 3) new axes/principal components for visualisation.\n",
    "    * Examine how much variance is explained by each principal component. \n",
    "    * Transform (project) the data onto the new axes. \n",
    "2. Fit the PCA model\n",
    "\n",
    "\n",
    "#### PCA in Code \n",
    "\n",
    "We use the scikit-learn library to perform PCA on a dataset. See an example with a created dataset X below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "X = np.array([[1,2],[1,1],[2,1],[3,2],[-4,-1],[-3,2],[1,-3]])\n",
    "pca_1 = PCA(n_components=1)\n",
    "pca_1.fit(X)\n",
    "pca_1.explained_variance_ratio_\n",
    "X_trans_1 = pca_1.transform(X)\n",
    "X_reduced_1 = pca_1.inverse_transform(X_trans_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning \n",
    "\n",
    "### Introduction \n",
    "\n",
    "#### What is reinforcement learning?\n",
    "\n",
    "Reinforcement learning is about mapping the state $s$ to an action $a$ and \"rewarding\" the function when it does the right thing by using a **reward function**. For example, this could be through assigning it a positive value for doing the right thing, and a large negative value for doing something wrong. \n",
    "\n",
    "Reinforcement learning has a variety of applications such as: \n",
    "* Controlling robots\n",
    "* Factory optimisation\n",
    "* Financial (stock) trading\n",
    "* Playing games\n",
    "  \n",
    "#### The Return\n",
    "\n",
    "The Return captures the idea that rewards you can get quicker are more attractive and appealing than rewards that take longer to achieve. \n",
    "\n",
    "$$\\text{Return for } R_n = R_1 + \\sum_{i=2}^n R_i (\\gamma)^i$$\n",
    "\n",
    "where: \n",
    "* $R_i$ is the reward at each state. \n",
    "* $\\gamma$ is the discount factor (a number close to 1).\n",
    "\n",
    "The Reward is based on which actions $a$ you take. \n",
    "\n",
    "In between the state and action, we have a policy $\\pi$ which job is to take an input state $s$ and map it to the action $a$. \n",
    "\n",
    "In summary, the goal of reinforcement learning is to find a policy $\\pi$ that tells you what action ($ a = \\pi(s)$) to take in every state(s) so as to maximise the return. \n",
    "\n",
    "The process of an agent $\\pi$ taking an action $a$ to get a state $s$ and reward $R$ is formally called a Markov Decision Process (MDP).\n",
    "\n",
    "### State Action Value Function\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The function can be defined as: $$\\begin{aligned} Q(s,a) = & \\text{ Return if you:} \\\\\n",
    "& \\text{   * Start in state } s \\\\\n",
    "& \\text{   * take action } a \\text{ (once)} \\\\\n",
    "& \\text{   * Then behave optimally after that} \\end{aligned}$$\n",
    "\n",
    "The optimal return from state $s$ is $\\max_a Q(s,a)$.\n",
    "\n",
    "The optimal action in state $s$ is the action $a$ that gives $\\max_a Q(s,a)$.\n",
    "\n",
    "#### Bellman Equation \n",
    "\n",
    "The Bellman Equation can be defined with the following parameters:\n",
    "* $Q(s,a) =$ state action value function\n",
    "* $s =$ current state\n",
    "* $R(s) =$ reward of current state\n",
    "* $a =$ current action \n",
    "* $s' =$ state you get to after taking action $a$\n",
    "* $a' =$ action that you take in state $s'$\n",
    "* $\\gamma =$ the discount factor\n",
    "\n",
    "The equation can be defined as follows:\n",
    "$$ Q(s,a) = R(s) + \\gamma \\max_{a'} Q(s',a')$$\n",
    "\n",
    "#### Random (stochastic) environment\n",
    "\n",
    "What happens if even though the system should choose the right action, a small percentage of the time the system \"slips\" and chooses the wrong action? This is what is known as a stochastic environment. \n",
    "\n",
    "To account for stochastic environments, we need to make an update for the return. Now Return becomes Expected Return = $E(R_1+\\gamma R_2 + \\gamma^2 R_3 + ...)$. \n",
    "\n",
    "Hence the Bellman equation becomes: \n",
    "$$ Q(s,a) = R(s) + \\gamma E[\\max_{a'} Q(s',a')]$$\n",
    "\n",
    "### Continuous State Spaces\n",
    "\n",
    "####Â Learning the state valued function \n",
    "\n",
    "We can implement reinforcement learning into a neural network to carry out deep reinforcement learning. In a state $s$, we will use a neural network to compute $Q(s,a)$ from a given input $\\vec{x} = \\begin{bmatrix} s \\\\ \\vdots \\\\ a \\end{bmatrix}$. The neural network will be trained to pick the action $a$ that maximises $Q(s,a)$.\n",
    "\n",
    "The learning algorithm will be:\n",
    "1. Initialise neural network randomly as a guess of $Q(s,a)$. \n",
    "2. Repeat {\n",
    "    * Take actions to get $(s,a,R(s),s')$.\n",
    "    * Store $10,000$ most recent tuples $(s,a,R(s),s')$.\n",
    "    * Train neural network: \n",
    "      * Create training set of $10,000$ examples using $x=(s,a)$ and $y=R(s) + \\max_{a'} Q(s',a')$\n",
    "      * Train $Q_{new}$ such that $Q_{new}(s,a) \\approx y$\n",
    "    * Set $Q = Q_{new}$\n",
    "\n",
    "This algorithm is sometimes called **DQN** or **Deep Q Network**.\n",
    "\n",
    "#### Refining the algorithm\n",
    "\n",
    "##### $\\epsilon$-greedy policy\n",
    "\n",
    "To choose actions whilst still learning, we have some options. In some state $s$ one can: \n",
    "1. Pick the action $a$ that maximises $Q(s,a)$\n",
    "2. \n",
    "  * With probability $0.95$, pick the action $a$ that maximises $Q(s,a)$, Greedy, \"Exploitation\"\n",
    "  * With probability $0.05$, pick an action $a$ randomly. \"Exploration\"\n",
    "\n",
    "The picking of an action $a$ randomly is called a **$\\epsilon$-greedy policy**. You should start with a high $\\epsilon$ value and gradually decrease it. \n",
    "\n",
    "##### Mini batch and soft updates\n",
    "\n",
    "The idea of mini batch gradient descent is to choose a batch of records to run gradient descent on, $m'$ which is smaller than $m$.\n",
    "\n",
    "Using mini batch will mean that we need to update the algorithm. The soft update method \n",
    "hopes to prevent $Q_{new}$ getting worse.\n",
    "For $Q_{new}$, we set $w_{new} = \\phi w_{new} + (1-\\phi)w$ and $b = \\phi b_{new} + (1 -\\phi) b$, where $\\phi$ is a hyper parameter chosen by you with $\\phi \\ll 1$ (e.g. $\\phi  = 0.01$). \n",
    "\n",
    "So the final algorithm is:\n",
    "1. Initialise neural network randomly as a guess of $Q(s,a)$. \n",
    "2. Repeat {\n",
    "    * Take actions to get $(s,a,R(s),s')$.\n",
    "    * Store $1000$ most recent tuples $(s,a,R(s),s')$.\n",
    "    * Train neural network: \n",
    "      * Create training set of $1000$ examples using $x=(s,a)$ and $y=R(s) + \\max_{a'} Q(s',a')$\n",
    "      * Train $Q_{new}$ such that $Q_{new}(s,a) \\approx y$\n",
    "    * Set $Q = Q_{new}$\n",
    "\n",
    "\n",
    "#### The state of reinforcement learning\n",
    "\n",
    "Reinforcement learning is an exciting set of technologies. However, there are a few limitations: \n",
    "* Much easier to get reinforcement learning to work in a simulation than a real robot. \n",
    "* Far fewer applications than supervised and unsupervised learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caffff98d144b369287ad0d4368df724c83d5682a99274de39cd1f951c63e42d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
